[paths]
vectors = "output/en_core_sci_md_vectors"
init_tok2vec = null
parser_tagger_path = "output/en_core_sci_md_parser_tagger/model-best"
dev_path = "assets/BC5CDR-IOB/devel.tsv"
train_path = "assets/BC5CDR-IOB/train.tsv"
vocab_path = "project_data/vocab_md.jsonl"
train = null
dev = null

[system]
gpu_allocator = null
seed = 0

[nlp]
lang = "en"
pipeline = ["tok2vec","tagger","attribute_ruler","lemmatizer","parser","med_ner","ner","negex"]
tokenizer = {"@tokenizers":"spacy.Tokenizer.v1"}
disabled = ["ner"]
before_creation = null
after_creation = null
after_pipeline_creation = null
batch_size = 1000

[components]

[components.attribute_ruler]
factory = "attribute_ruler"
scorer = {"@scorers":"spacy.attribute_ruler_scorer.v1"}
validate = false

[components.lemmatizer]
factory = "lemmatizer"
mode = "rule"
model = null
overwrite = false
scorer = {"@scorers":"spacy.lemmatizer_scorer.v1"}

[components.med_ner]
factory = "ner"
incorrect_spans_key = null
moves = null
scorer = {"@scorers":"spacy.ner_scorer.v1"}
update_with_oracle_cut_size = 100

[components.med_ner.model]
@architectures = "spacy.TransitionBasedParser.v2"
state_type = "ner"
extra_state_tokens = false
hidden_width = 64
maxout_pieces = 2
use_upper = true
nO = null

[components.med_ner.model.tok2vec]
@architectures = "spacy.Tok2Vec.v2"

[components.med_ner.model.tok2vec.embed]
@architectures = "spacy.MultiHashEmbed.v2"
width = 256
attrs = ["ORTH","SHAPE"]
rows = [5000,2500]
include_static_vectors = true

[components.med_ner.model.tok2vec.encode]
@architectures = "spacy.MaxoutWindowEncoder.v2"
width = 256
depth = 8
window_size = 1
maxout_pieces = 3

[components.negex]
factory = "negex"
chunk_prefix = []
ent_types = ["SA","DA","ABDOMINAL","DIABETES","CAD","MD"]
extension_name = "negex"

[components.negex.neg_termset]
pseudo_negations = ["although","not able to be","might not","not drain","without any further","no further","not ruled out","not only","without difficulty","without further","no interval change","no change","not cause","not certain if","not rule out","no significant interval change","not extend","not necessarily","not certain whether","no definite change","no increase","no suspicious change","no significant change","gram negative","not been ruled out"]
preceding_negations = ["negative","never developed","isn't","no","aren't","denied","rule patient out","sister","rule him out","out","free of","wasn't","never","cannot","without","no signs of","didn't","no cause of","mom","without sign of","no complaints of","couldnt","r/o","doubt","declined","ro","isnt","ruled the patient out","versus","can't","ruled out","weren't","sometimes","dad","negative for","denying","did not exhibit","ruled him out","no evidence of","evaluate for","not","brother","possibility","symptoms atypical","cant","without any reactions or signs of","unrevealing","dont","denies","mother","limited","don't","father","couldn't","spouse","absence of","ruled her out","patient was not","no sign of","werent","never had","doesnt","wasnt","not demonstrate","son","perhaps","wife","without signs of","rule her out","doesn't","without indication of","didnt","fails to reveal","arent","rule the patient out","rules out","rule out","ruled patient out","husband"]
following_negations = ["were ruled out","son","free","perhaps","denies","mom","mother","stable","wife","declined","father","spouse","was not","was ruled out","no","sister","Unknown","recommended","out","unlikely","werent","weren't","dad","sometimes","wasn't","wasnt","brother","husband","were not"]
termination = ["as there are","although","reasons for","origins for","secondary to","other possibilities of","which","trigger event for","however","reasons of","sources for","causes of","reason of","except","origins of","reason for","etiology of","origin of","cause for","causes for","origin for","after","source for","cause of","nevertheless","sources of","aside from","still","etiology for","apart from","involving","but","yet","source of","though"]

[components.ner]
factory = "ner"
incorrect_spans_key = null
moves = null
scorer = {"@scorers":"spacy.ner_scorer.v1"}
update_with_oracle_cut_size = 100

[components.ner.model]
@architectures = "spacy.TransitionBasedParser.v2"
state_type = "ner"
extra_state_tokens = false
hidden_width = 128
maxout_pieces = 3
use_upper = true
nO = null

[components.ner.model.tok2vec]
@architectures = "spacy.Tok2Vec.v2"

[components.ner.model.tok2vec.embed]
@architectures = "spacy.MultiHashEmbed.v2"
width = 96
attrs = ["NORM","PREFIX","SUFFIX","SHAPE","SPACY"]
rows = [5000,2500,2500,2500,100]
include_static_vectors = ${vars.include_static_vectors}

[components.ner.model.tok2vec.encode]
@architectures = "spacy.MaxoutWindowEncoder.v2"
width = 96
depth = 4
window_size = 1
maxout_pieces = 3

[components.parser]
factory = "parser"
learn_tokens = false
min_action_freq = 30
moves = null
scorer = {"@scorers":"spacy.parser_scorer.v1"}
update_with_oracle_cut_size = 100

[components.parser.model]
@architectures = "spacy.TransitionBasedParser.v2"
state_type = "parser"
extra_state_tokens = false
hidden_width = 128
maxout_pieces = 3
use_upper = true
nO = null

[components.parser.model.tok2vec]
@architectures = "spacy.Tok2VecListener.v1"
width = 96
upstream = "*"

[components.tagger]
factory = "tagger"
neg_prefix = "!"
overwrite = false
scorer = {"@scorers":"spacy.tagger_scorer.v1"}

[components.tagger.model]
@architectures = "spacy.Tagger.v1"
nO = null

[components.tagger.model.tok2vec]
@architectures = "spacy.Tok2VecListener.v1"
width = 96
upstream = "*"

[components.tok2vec]
factory = "tok2vec"

[components.tok2vec.model]
@architectures = "spacy.Tok2Vec.v2"

[components.tok2vec.model.embed]
@architectures = "spacy.MultiHashEmbed.v2"
width = 96
attrs = ["NORM","PREFIX","SUFFIX","SHAPE","SPACY"]
rows = [5000,2500,2500,2500,100]
include_static_vectors = "True"

[components.tok2vec.model.encode]
@architectures = "spacy.MaxoutWindowEncoder.v2"
width = 96
depth = 4
window_size = 1
maxout_pieces = 3

[corpora]

[corpora.dev]
@readers = "specialized_ner_reader"
file_path = ${paths.dev_path}

[corpora.train]
@readers = "specialized_ner_reader"
file_path = ${paths.train_path}

[training]
dev_corpus = "corpora.dev"
train_corpus = "corpora.train"
seed = ${system.seed}
gpu_allocator = ${system.gpu_allocator}
dropout = 0.1
accumulate_gradient = 1
patience = 0
max_epochs = 7
max_steps = 0
eval_frequency = 500
frozen_components = ["tok2vec","parser","tagger","attribute_ruler","lemmatizer"]
before_to_disk = null
annotating_components = []

[training.batcher]
@batchers = "spacy.batch_by_sequence.v1"
get_length = null

[training.batcher.size]
@schedules = "compounding.v1"
start = 1
stop = 32
compound = 1.001
t = 0.0

[training.logger]
@loggers = "spacy.ConsoleLogger.v1"
progress_bar = true

[training.optimizer]
@optimizers = "Adam.v1"
beta1 = 0.9
beta2 = 0.999
L2_is_weight_decay = true
L2 = 0.01
grad_clip = 1.0
use_averages = false
eps = 0.00000001
learn_rate = 0.001

[training.score_weights]
tag_acc = null
lemma_acc = 0.5
dep_uas = null
dep_las = null
dep_las_per_type = null
sents_p = null
sents_r = null
sents_f = null
ents_f = 0.5
ents_p = 0.0
ents_r = 0.0
ents_per_type = null

[pretraining]

[initialize]
vectors = ${paths.vectors}
init_tok2vec = ${paths.init_tok2vec}
vocab_data = ${paths.vocab_path}
lookups = null
before_init = {"@callbacks":"replace_tokenizer"}
after_init = null

[initialize.components]

[initialize.tokenizer]

[vars]
include_static_vectors = "True"